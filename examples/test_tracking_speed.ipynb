{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Pynta program\n",
    "\n",
    "This notebook was developed for testing the time limitations of the Pynta program. The design of Pynta allows it to run without a GUI, therefore this notebook can also be used as a reference for performing experiments. \n",
    "\n",
    "Data will be simulated data, through the module called SimBrownian. \n",
    "**Important**: Data is very clean, no background, no noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import Queue, Process, Event\n",
    "from threading import Thread\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "from pynta import BASE_DIR\n",
    "from pynta.model.experiment.nanoparticle_tracking.np_tracking import NPTracking\n",
    "from pynta.model.cameras.simulate_brownian import SimBrownian\n",
    "from pynta.model.experiment.subscriber import subscribe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters that can change in the evaluation\n",
    "\n",
    "The main parameter to change in this notebook is the number of particles in an image, since this is most likely the one that is going to affect the most the final outcome for the tracking algorithm. \n",
    "\n",
    "It is important to note that other parameters such as particle diameter (in pixels) also has a big role in total execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_to_accumulate = 0  # How many frames SimBrownian will accumulate\n",
    "num_particles = 50  # Number of particles in a given image\n",
    "pcle_diameter = 5  # In pixels\n",
    "magnification = 30 # Magnification of the microscope for the simulation of the brownian motion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before running the cell below**, it is important to check whether the experiment has finalized correctly. If it hasn't, ZMQ will complain about the port being in use, but this warning will appear on the console, not on the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SimBrownian.num_particles = num_particles\n",
    "SimBrownian.frames_to_accumulate = frames_to_accumulate\n",
    "SimBrownian.magnification = magnification\n",
    "config_file = os.path.join(BASE_DIR, 'util', 'example_config.yml')\n",
    "exp = NPTracking(config_file)\n",
    "exp.initialize_camera()\n",
    "exp.config['tracking']['locate']['diameter'] = pcle_diameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start free run and tracking\n",
    "We will start both tracking and the free run of the camera. SimBrownian has the possibility of accumulating frames into memory in order to loop through them instead of keep simulating data. If this parameter is set to a value other than 0, then you have to be careful with the fact that statistics will make no sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.start_tracking()\n",
    "exp.start_free_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consume location timing information\n",
    "In order to check the time it takes to perform a localization, we have developed the function below, which will be listening for a specific topic. \n",
    "\n",
    "This is a great example on how to extend the functionality of Pynta without altering the code base. In this case, we listent on a socket and push the data to a Queue. We perform this work-around in order to get the data out of the multi-processing realm, but you are free to follow different paths.\n",
    "\n",
    "In order to stop the thread, you will need to do:\n",
    "```\n",
    "event.set()\n",
    "```\n",
    "\n",
    "And you will be able to check whether it worked by:\n",
    "```\n",
    "p.is_alive()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_queue = Queue()\n",
    "event = Event()\n",
    "\n",
    "def consume_location(queue, event):\n",
    "    socket = subscribe(5555, 'locations_time')\n",
    "    sleep(1)\n",
    "    while not event.is_set():\n",
    "        topic = socket.recv_string()\n",
    "        data = socket.recv_pyobj()\n",
    "        queue.put(data)\n",
    "    socket.close()\n",
    "p = Thread(target=consume_location, args=[loc_queue, event])\n",
    "p.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finishing\n",
    "Once you have finished with the measurements, you can stop the experiment and the ``consume_locations`` thread. It is not mandatory to do it at this time, you can first inspect the data and decide whether you would like to generate more or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sleep(30)\n",
    "exp.stop_tracking()\n",
    "exp.stop_free_run()\n",
    "exp.finalize()\n",
    "event.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the temporal data\n",
    "\n",
    "We will need to get the data from a queue into a usable format. We are accumulating information on the time it took to perfom a location and the number of particles on each frame. We can read the queue and then transform the data to a numpy array. First, let's create an empty list, be sure not to overwrite it, or you will loose the information stored in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp Data length:  0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-4f9896443f67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Temp Data length: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mt_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Average time: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Average particles: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "while not loc_queue.empty():\n",
    "    temp_data.append(loc_queue.get())\n",
    "print('Temp Data length: ', len(temp_data))\n",
    "t_data = np.array(temp_data)\n",
    "print('Average time: ', np.mean(t_data[:,0]))\n",
    "print('Average particles: ', np.mean(t_data[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the data\n",
    "\n",
    "If we want to look at the data, we can make a plot, for example of the number of particles detected and the time it took, or a histogram of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t_data[:,1], 1000*t_data[:,0], 'o')\n",
    "plt.xlabel('Number of particles in the frame')\n",
    "plt.ylabel('Time to localize particles (ms)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(1000*t_data[t_data[:,0]<.4,0], bins=50)\n",
    "plt.xlabel('Time to process a frame (ms)')\n",
    "plt.ylabel('Number of frames')\n",
    "plt.title(f'Number of particles: {num_particles}')\n",
    "plt.savefig(f'hist_{num_particles}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(f'{num_particles}-{pcle_diameter}-1920x1080_particles.dat', t_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze all the generated data\n",
    "\n",
    "## First, let's see the correlation with the number of particles on each frame\n",
    "\n",
    "Error bars are defined as the standard deviation of the distribution of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcles = [10, 30, 50, 70, 90, 110, 130]\n",
    "pcle10 = np.loadtxt('10-3_particles.dat')\n",
    "pcle30 = np.loadtxt('30-3_particles.dat')\n",
    "pcle50 = np.loadtxt('50-3_particles.dat')\n",
    "pcle70 = np.loadtxt('70-3_particles.dat')\n",
    "pcle90 = np.loadtxt('90-3_particles.dat')\n",
    "pcle110 = np.loadtxt('110-3_particles.dat')\n",
    "pcle130 = np.loadtxt('130-3_particles.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p10 = np.mean(pcle10[:,0])*1000\n",
    "p30 = np.mean(pcle30[:,0])*1000\n",
    "p50 = np.mean(pcle50[:,0])*1000\n",
    "p70 = np.mean(pcle70[:,0])*1000\n",
    "p90 = np.mean(pcle90[:,0])*1000\n",
    "p110 = np.mean(pcle110[:,0])*1000\n",
    "p130 = np.mean(pcle130[:,0])*1000\n",
    "p10err = np.std(pcle10[:, 0]*1000)\n",
    "p30err = np.std(pcle30[:, 0]*1000)\n",
    "p50err = np.std(pcle50[:, 0]*1000)\n",
    "p70err = np.std(pcle70[:, 0]*1000)\n",
    "p90err = np.std(pcle90[:, 0]*1000)\n",
    "p110err = np.std(pcle110[:, 0]*1000)\n",
    "p130err = np.std(pcle130[:, 0]*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(pcles, [p10, p30, p50, p70, p90, p110, p130], yerr=[p10err, p30err, p50err, p70err, p90err, p110err, p130err ], fmt='o')\n",
    "plt.ylabel('Average frame time (ms)')\n",
    "plt.xlabel('Number of particles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation with diameter\n",
    "The algorithm for identifying particles requires to determine the diameter of the particles (approximately), a-priory. This is explained in the original paper. And indeed, the bigger the particle, the slower the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diameters = [3, 5, 7, 9, 11, 13]\n",
    "diam3 = np.loadtxt('50-3_particles.dat')\n",
    "diam5 = np.loadtxt('50-5_particles.dat')\n",
    "diam7 = np.loadtxt('50-7_particles.dat')\n",
    "diam9 = np.loadtxt('50-9_particles.dat')\n",
    "diam11 = np.loadtxt('50-11_particles.dat')\n",
    "diam13 = np.loadtxt('50-13_particles.dat')\n",
    "d3 = np.mean(diam3[:,0])*1000\n",
    "d5 = np.mean(diam5[:,0])*1000\n",
    "d7 = np.mean(diam7[:,0])*1000\n",
    "d9 = np.mean(diam9[:,0])*1000\n",
    "d11 = np.mean(diam11[:,0])*1000\n",
    "d13 = np.mean(diam13[:,0])*1000\n",
    "d3err = np.std(diam3[:, 0]*1000)\n",
    "d5err = np.std(diam5[:, 0]*1000)\n",
    "d7err = np.std(diam7[:, 0]*1000)\n",
    "d9err = np.std(diam9[:, 0]*1000)\n",
    "d11err = np.std(diam11[:, 0]*1000)\n",
    "d13err = np.std(diam13[:, 0]*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(diameters, [d3, d5, d7, d9, d11, d13], yerr=[d3err, d5err, d7err, d9err, d11err, d13err], fmt='o')\n",
    "plt.ylabel('Average frame time (ms)')\n",
    "plt.xlabel('Diameter of the particles (px)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependence with frame size\n",
    "Some frames were generated to see if the timing depends on the frame size. Some standard sizes (i.e. fullHD, for example) were taken in consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pixels = np.array([800*600, 1200*800, 1600*1200, 1920*1080])/1000000\n",
    "times1 = np.loadtxt('50-5_particles.dat')\n",
    "times2 = np.loadtxt('50-5-1200x800_particles.dat')\n",
    "times3 = np.loadtxt('50-5-1600x1200_particles.dat')\n",
    "times4 = np.loadtxt('50-5-1920x1080_particles.dat')\n",
    "t1 = np.mean(times1[:,0])*1000\n",
    "t2 = np.mean(times2[:,0])*1000\n",
    "t3 = np.mean(times3[:,0])*1000\n",
    "t4 = np.mean(times4[:,0])*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(num_pixels, [t1, t2, t3, t4], 'o')\n",
    "plt.xlabel('Frame size (Mpix)')\n",
    "plt.ylabel('Frame processing time (ms)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
